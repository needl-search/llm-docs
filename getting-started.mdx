---
title: "Getting Started"
description: "Getting started with Needl only takes understanding a few key concepts."
---

One common use case for LLMs is to answer questions about a document or collection of documents. In this case a simple prompt template you could use might be something like this:

```
Using the following documents, answer the question.

{{ documents }}

Question: {{ question }}
```

The variables 'documents' and 'question' depend on the documents we want to answer questions about and the question we have.

Using Needl, we can evaluate the effectiveness of this prompt, and compare different variations until we find one that consistently produces good results.


## Suites

The first step to using Needl is to create a Test Suite. Suites are in a nutshell just collections of test cases. By testing new prompt and model variations against a range of test cases, we can get a better sense of what prompt produces the best results rather than just guessing. 

For our example above, let's create a Suite with the following parameters:

```json
{
    "name": "Document Answering Test Cases"
    "required_inputs": ["documents", "question"]
}
```

`required_inputs` refers to the variables contained within the prompt. This ensure that all test cases included will work with any new prompts we test. If our test cases did not include any documents for example, it would not make sense in this instance.



## Test Cases
Now that we have a suite, we need to add some test cases to it to get started. A test case is a possible usage of our prompt that we want to validate produces our desired output. For example for our use case above, a simple test case might look like:

```json
{
    "input_variables": {
        "documents": "The Acme Corporation was founded in 2021",
        "question": "When was the acme corporation founded?"
    },
    "evaluation_criteria": [
        {
            "match": "2021"
            "weight": 1.0
        }
    ]
}
```

`input_variables` provides the values for the variable values in the prompt. In this case, our `documents` that we want to answer some questions about is some simple information about the Acme Corporation and our `question` is a simple question to answer.

`evaluation_criteria` lets Needl know what factors to grade the LLM's response on. In this case, we simply want to make sure the string '2021' is included in the output. This is the crucial part of the Needl platform. There are several different ways you can evaluate a prompt.

    - `match`: A string that should be contained in the LLM response exactly
    - `regex`: A regex pattern that the LLM response should match
    - `tone`: The tone you want the response to have
    - `task`: Describe the task you want the LLM to perform. We will evaluate whether or not the task was performed or not according to the specification
    - `ground_truth`: Provide a response that you consider to be the correct response to the question. We will compare your provided ground truth to the LLM output and determine if there is hallucinated information, or missing information

For each criteria, you can also include a `weight` parameter. If you have multiple evaluation criteria, and you want the tone of the response to affect the score less than whether the response is accurate based on the ground truth, you can give the ground truth criteria a higher weight.

## Runs

Now that we've added a couple of test cases, we can run them against different prompts! Runs are variations of the prompt and model combinations that you want to test. When you create a run, Needl will run all tests in the targeted suite on the prompt iteration. Runs give you several factors to adjust:

```json
{
    "prompt": [
        {
            "role": "system"
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "Answer the question below. Do not make up information. Only reference information included in the documents provided.\n\n{{ documents }}\n\nQuestion: {{ question }}"
        }
    ],
    "model": "gpt-4",
    "temperature": 0
}
```

Needl will run the tests in the targeted suite, evaluate the criteria, and output a score which you can then use to determine what variation of your prompt performs the best. You can drill down to see exactly which test cases are problematic, or use the overarching scores to determine what prompt is performing best. Needl will also keep track of other factors such as latency, and number of tokens so you can take into account other factors when making your decision.